# VPS Migration Checklist

## Pre-Migration (Local Machine)

### âœ… Completed
- [x] Cataloged all datasets (5,208 total)
  - 5,134 local-only (need upload)
  - 74 HuggingFace-accessible (direct to S3)
  - 0 Kaggle-accessible
  - 0 URL-accessible
- [x] Created dataset accessibility catalog
- [x] Created S3 download scripts
- [x] Created migration documentation
- [x] Created tarball with all necessary files

### ðŸ“‹ Before Uploading Tarball

1. **Verify Tarball**
   ```bash
   tar -tzf training_ready_vps_*.tar.gz | head -20
   ```

2. **Check Size**
   - Tarball should be ~1-2GB (code + configs only, no datasets)
   - Datasets will be downloaded/uploaded separately

3. **Prepare AWS Credentials**
   - Have AWS_ACCESS_KEY_ID ready
   - Have AWS_SECRET_ACCESS_KEY ready
   - Know S3 bucket name
   - Know AWS region

## Migration Steps (VPS)

### 1. Extract Tarball
```bash
cd /path/to/workspace
tar -xzf training_ready_vps_*.tar.gz
cd ai/training_ready
```

### 2. Read Documentation
```bash
cat GET_UP_TO_SPEED.md      # Context and current status
cat VPS_MIGRATION_GUIDE.md   # Step-by-step migration
cat ENV_QUICKSTART.md        # Environment setup
```

### 3. Set Up Environment
```bash
# Install dependencies
./install_dependencies.sh

# Or manually
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
uv pip install boto3 datasets kaggle requests
```

### 4. Configure AWS/S3
```bash
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"
export AWS_DEFAULT_REGION="us-east-1"
export S3_BUCKET="your-bucket-name"
```

### 5. Verify Catalog
```bash
# Check accessibility catalog
cat scripts/output/dataset_accessibility_catalog.json | jq '.summary'

# Expected:
# - total: 5208
# - local_only: 5134
# - huggingface: 74
```

### 6. Download Remote Datasets to S3
```bash
# Download HuggingFace datasets directly to S3
python3 scripts/download_to_s3.py \
  --bucket $S3_BUCKET \
  --catalog scripts/output/dataset_accessibility_catalog.json
```

### 7. Upload Local-Only Datasets
```bash
# From your LOCAL machine (not VPS), upload local-only datasets
# Use the upload list generated by catalog script
aws s3 sync \
  --exclude "*" \
  --include-from ai/training_ready/scripts/output/local_only_upload_list.txt \
  / \
  s3://$S3_BUCKET/datasets/local/
```

**Note**: This step must be done from your local machine where the files actually exist.

### 8. Update Manifest with S3 Paths
```bash
# After all datasets are in S3, update manifest paths
# (Script to be created or manual update)
```

### 9. Test Pipeline
```bash
# Test data processing pipeline with S3 paths
uv run python3 scripts/prepare_training_data.py --test
```

## Post-Migration Verification

### âœ… Checklist
- [ ] Tarball extracted successfully
- [ ] Dependencies installed
- [ ] AWS credentials configured
- [ ] S3 bucket accessible
- [ ] Remote datasets downloaded to S3 (74 HuggingFace)
- [ ] Local-only datasets uploaded to S3 (5,134 files)
- [ ] Manifest updated with S3 paths
- [ ] Pipeline can read from S3
- [ ] Test run successful

## Troubleshooting

### Tarball Issues
- **Too large**: Check if datasets were accidentally included (should only be code/configs)
- **Missing files**: Verify all essential pipeline files are included

### AWS/S3 Issues
- **Credentials**: Verify AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set
- **Permissions**: Ensure IAM policy allows S3 read/write
- **Bucket**: Verify bucket name and region are correct

### Import Errors
- **Missing modules**: Run `./install_dependencies.sh` again
- **Path issues**: Ensure `ai/dataset_pipeline/` is in the tarball
- **Python path**: Use `uv run python3` to ensure correct environment

### Download Issues
- **HuggingFace**: Set `HF_TOKEN` if using private datasets
- **Kaggle**: Configure `~/.kaggle/kaggle.json`
- **Network**: Check VPS internet connection

## File Locations

### On VPS (After Extraction)
```
/path/to/workspace/
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ training_ready/          # Main directory
â”‚   â”‚   â”œâ”€â”€ scripts/            # All scripts
â”‚   â”‚   â”œâ”€â”€ TRAINING_MANIFEST.json
â”‚   â”‚   â”œâ”€â”€ GET_UP_TO_SPEED.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ dataset_pipeline/        # Dependencies
â””â”€â”€ PACKAGE_MANIFEST.txt
```

### S3 Structure (After Migration)
```
s3://your-bucket/
â””â”€â”€ datasets/
    â”œâ”€â”€ huggingface/          # 74 datasets
    â”œâ”€â”€ kaggle/               # 0 datasets (none found)
    â”œâ”€â”€ urls/                 # 0 datasets (none found)
    â””â”€â”€ local/                # 5,134 local-only files
```

## Next Steps After Migration

1. âœ… Verify all datasets are in S3
2. âœ… Update manifest with S3 paths
3. âœ… Run full data processing pipeline
4. âœ… Generate final training datasets
5. âœ… Begin training preparation

