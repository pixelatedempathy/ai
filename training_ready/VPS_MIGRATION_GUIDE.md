# VPS Migration Guide: Training Consolidation

## Overview

This guide helps transition the training consolidation work from local machine to VPS with OVH Cloud S3-compatible storage. Datasets will be downloaded directly from HuggingFace/Kaggle to OVH object storage, bypassing local storage.

## Quick Start

### 1. Extract Tarball on VPS
```bash
cd /path/to/workspace
tar -xzf training_ready_vps.tar.gz
cd ai/training_ready
```

### 2. Set Up Environment
```bash
# Install dependencies
./install_dependencies.sh

# Or manually
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
uv pip install boto3 datasets kaggle requests
```

### 3. Configure OVH S3 (object storage)
```bash
export OVH_S3_BUCKET="pixel-data"
export OVH_S3_ENDPOINT="https://s3.us-east-va.io.cloud.ovh.us"
export OVH_S3_REGION="us-east-va"
export OVH_S3_ACCESS_KEY="your-ovh-access-key"
export OVH_S3_SECRET_KEY="your-ovh-secret-key"

# For scripts that still use S3_BUCKET
export S3_BUCKET="$OVH_S3_BUCKET"
```

### 4. Catalog Datasets
```bash
# Identify what's local-only vs remote-accessible
python3 scripts/catalog_local_only_datasets.py
```

### 5. Download Remote Datasets to OVH S3
```bash
# Download all remote-accessible datasets directly to OVH S3
python3 scripts/download_to_s3.py \
  --bucket $S3_BUCKET \
  --catalog scripts/output/dataset_accessibility_catalog.json
```

### 6. Upload Local-Only Datasets
```bash
# Upload local-only datasets from your local machine to OVH S3
# (Use the upload_list.txt generated by catalog script)
aws s3 sync \
  --endpoint-url "$OVH_S3_ENDPOINT" \
  --exclude "*" \
  --include-from scripts/output/local_only_upload_list.txt \
  / \
  s3://$S3_BUCKET/datasets/local/
```

## Dataset Strategy

### Remote-Accessible (Direct to S3)
- **HuggingFace**: Downloaded directly via `datasets` library → S3
- **Kaggle**: Downloaded via Kaggle API → S3
- **URLs**: Downloaded via HTTP → S3

### Local-Only (Must Upload)
- Files only on your local machine
- Custom datasets not available remotely
- Generated datasets from local pipelines

## File Structure

```
ai/training_ready/
├── scripts/
│   ├── catalog_local_only_datasets.py    # Identify local vs remote
│   ├── download_to_s3.py                 # Download remote datasets to S3
│   ├── explore_directories.py            # Directory exploration
│   ├── generate_manifest.py              # Manifest generation
│   └── prepare_training_data.py         # Data processing pipeline
├── TRAINING_MANIFEST.json               # Complete asset catalog
├── install_dependencies.sh               # Environment setup
└── VPS_MIGRATION_GUIDE.md               # This file
```

## S3 Structure (OVH object storage)

```
s3://pixel-data/
└── datasets/
    ├── huggingface/          # HF datasets
    ├── kaggle/               # Kaggle datasets
    ├── urls/                 # URL downloads
    └── local/                # Local-only uploads
```

## Environment Variables

```bash
# OVH S3
export OVH_S3_BUCKET="pixel-data"
export OVH_S3_ENDPOINT="https://s3.us-east-va.io.cloud.ovh.us"
export OVH_S3_REGION="us-east-va"
export OVH_S3_ACCESS_KEY="..."
export OVH_S3_SECRET_KEY="..."
export S3_BUCKET="$OVH_S3_BUCKET"

# Kaggle (if using)
export KAGGLE_USERNAME="your-username"
export KAGGLE_KEY="your-api-key"

# HuggingFace (if using private datasets)
export HF_TOKEN="your-hf-token"
```

## Next Steps

1. ✅ Extract tarball on VPS
2. ✅ Install dependencies
3. ✅ Configure OVH S3 credentials
4. ✅ Catalog datasets (identify local-only)
5. ✅ Download remote datasets to S3
6. ✅ Upload local-only datasets from local machine
7. ✅ Update manifest with S3 paths
8. ✅ Run data processing pipeline (reads from S3)

## Troubleshooting

### OVH S3 Credentials
- Ensure `OVH_S3_ACCESS_KEY` and `OVH_S3_SECRET_KEY` are set
- Verify bucket permissions (read/write) on `pixel-data`

### Kaggle API
- Download `kaggle.json` from Kaggle account settings
- Place in `~/.kaggle/kaggle.json`

### HuggingFace
- For private datasets, set `HF_TOKEN` environment variable

### Slow Uploads
- Use `aws s3 sync` with `--exclude` patterns for selective upload
- Consider using S3 Transfer Acceleration
- Upload during off-peak hours


