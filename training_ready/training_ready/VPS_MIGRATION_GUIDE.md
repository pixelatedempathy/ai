# VPS Migration Guide: Training Consolidation

## Overview

This guide helps transition the training consolidation work from local machine to VPS with S3 storage. Datasets will be downloaded directly from HuggingFace/Kaggle to S3, bypassing local storage.

## Quick Start

### 1. Extract Tarball on VPS
```bash
cd /path/to/workspace
tar -xzf training_ready_vps.tar.gz
cd ai/training_ready
```

### 2. Set Up Environment
```bash
# Install dependencies
./install_dependencies.sh

# Or manually
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
uv pip install boto3 datasets kaggle requests
```

### 3. Configure AWS/S3
```bash
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"
export AWS_DEFAULT_REGION="us-east-1"  # or your region
export S3_BUCKET="your-bucket-name"
```

### 4. Catalog Datasets
```bash
# Identify what's local-only vs remote-accessible
python3 scripts/catalog_local_only_datasets.py
```

### 5. Download Remote Datasets to S3
```bash
# Download all remote-accessible datasets directly to S3
python3 scripts/download_to_s3.py \
  --bucket $S3_BUCKET \
  --catalog scripts/output/dataset_accessibility_catalog.json
```

### 6. Upload Local-Only Datasets
```bash
# Upload local-only datasets from your local machine
# (Use the upload_list.txt generated by catalog script)
aws s3 sync --exclude "*" --include-from scripts/output/local_only_upload_list.txt s3://$S3_BUCKET/datasets/local/
```

## Dataset Strategy

### Remote-Accessible (Direct to S3)
- **HuggingFace**: Downloaded directly via `datasets` library → S3
- **Kaggle**: Downloaded via Kaggle API → S3
- **URLs**: Downloaded via HTTP → S3

### Local-Only (Must Upload)
- Files only on your local machine
- Custom datasets not available remotely
- Generated datasets from local pipelines

## File Structure

```
ai/training_ready/
├── scripts/
│   ├── catalog_local_only_datasets.py    # Identify local vs remote
│   ├── download_to_s3.py                 # Download remote datasets to S3
│   ├── explore_directories.py            # Directory exploration
│   ├── generate_manifest.py              # Manifest generation
│   └── prepare_training_data.py         # Data processing pipeline
├── TRAINING_MANIFEST.json               # Complete asset catalog
├── install_dependencies.sh               # Environment setup
└── VPS_MIGRATION_GUIDE.md               # This file
```

## S3 Structure

```
s3://your-bucket/
└── datasets/
    ├── huggingface/          # HF datasets
    ├── kaggle/               # Kaggle datasets
    ├── urls/                 # URL downloads
    └── local/                # Local-only uploads
```

## Environment Variables

```bash
# AWS/S3
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
export AWS_DEFAULT_REGION="us-east-1"
export S3_BUCKET="your-bucket-name"

# Kaggle (if using)
export KAGGLE_USERNAME="your-username"
export KAGGLE_KEY="your-api-key"

# HuggingFace (if using private datasets)
export HF_TOKEN="your-hf-token"
```

## Next Steps

1. ✅ Extract tarball on VPS
2. ✅ Install dependencies
3. ✅ Configure AWS credentials
4. ✅ Catalog datasets (identify local-only)
5. ✅ Download remote datasets to S3
6. ✅ Upload local-only datasets from local machine
7. ✅ Update manifest with S3 paths
8. ✅ Run data processing pipeline (reads from S3)

## Troubleshooting

### AWS Credentials
- Ensure `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are set
- Verify bucket permissions (read/write)

### Kaggle API
- Download `kaggle.json` from Kaggle account settings
- Place in `~/.kaggle/kaggle.json`

### HuggingFace
- For private datasets, set `HF_TOKEN` environment variable

### Slow Uploads
- Use `aws s3 sync` with `--exclude` patterns for selective upload
- Consider using S3 Transfer Acceleration
- Upload during off-peak hours


