# OVHcloud AI Deploy Dockerfile for Pixelated Empathy Inference
# Optimized for low-latency inference on L4 GPUs

ARG PYTORCH_VERSION=2.2.0
ARG CUDA_VERSION=12.1

FROM pytorch/pytorch:${PYTORCH_VERSION}-cuda${CUDA_VERSION}-cudnn8-runtime

LABEL maintainer="Pixelated Empathy Team"
LABEL description="OVHcloud AI Deploy inference image for therapeutic AI"
LABEL version="1.0.0"

ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    DEBIAN_FRONTEND=noninteractive \
    CUDA_DEVICE_ORDER=PCI_BUS_ID \
    MODEL_DIR=/models \
    TRANSFORMERS_CACHE=/tmp/transformers_cache \
    HF_HOME=/tmp/huggingface

RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

WORKDIR /app

RUN pip install --upgrade pip && \
    pip install \
    torch>=2.2.0 \
    transformers>=4.42.0 \
    accelerate>=0.33.0 \
    peft>=0.11.0 \
    sentencepiece>=0.1.99 \
    einops>=0.7.0 \
    safetensors>=0.4.0 \
    fastapi>=0.115.0 \
    uvicorn[standard]>=0.34.0 \
    pydantic>=2.0.0 \
    httpx>=0.28.0 \
    numpy>=1.24.0 \
    scipy>=1.10.0

COPY lightning/moe_architecture.py /app/
COPY lightning/inference_service.py /app/
COPY ovh/inference_server.py /app/

RUN mkdir -p /models /tmp/transformers_cache /tmp/huggingface

RUN groupadd -r inference && useradd -r -g inference -u 1000 inference && \
    chown -R inference:inference /app /models /tmp

USER inference

EXPOSE 8080

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

CMD ["python", "-m", "uvicorn", "inference_server:app", "--host", "0.0.0.0", "--port", "8080"]
