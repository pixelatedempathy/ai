#!/usr/bin/env python3
"""
GROUP F FINAL COMPREHENSIVE AUDIT REPORT
Combines all audit results for complete Group F assessment
"""

import json
import logging
from datetime import datetime
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - FINAL_AUDIT - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def create_final_audit_report():
    """Create final comprehensive audit report."""
    logger.critical("üìã CREATING FINAL GROUP F AUDIT REPORT")
    
    # Load both audit reports
    comprehensive_report_path = Path('/home/vivi/pixelated/ai/GROUP_F_COMPREHENSIVE_AUDIT_REPORT.json')
    extended_report_path = Path('/home/vivi/pixelated/ai/GROUP_F_EXTENDED_AUDIT_REPORT.json')
    
    comprehensive_data = {}
    extended_data = {}
    
    if comprehensive_report_path.exists():
        with open(comprehensive_report_path, 'r') as f:
            comprehensive_data = json.load(f)
    
    if extended_report_path.exists():
        with open(extended_report_path, 'r') as f:
            extended_data = json.load(f)
    
    # Combine task scores
    all_task_scores = {}
    all_task_scores.update(comprehensive_data.get('task_scores', {}))
    all_task_scores.update(extended_data.get('task_scores', {}))
    
    # Combine detailed results
    all_detailed_results = {}
    all_detailed_results.update(comprehensive_data.get('detailed_results', {}))
    all_detailed_results.update(extended_data.get('detailed_results', {}))
    
    # Calculate overall statistics
    total_tasks = len(all_task_scores)
    overall_score = sum(all_task_scores.values()) / total_tasks if total_tasks > 0 else 0
    
    tasks_excellent = sum(1 for score in all_task_scores.values() if score >= 90)
    tasks_good = sum(1 for score in all_task_scores.values() if score >= 75 and score < 90)
    tasks_needs_improvement = sum(1 for score in all_task_scores.values() if score >= 50 and score < 75)
    tasks_critical = sum(1 for score in all_task_scores.values() if score < 50)
    
    # Task mapping
    task_names = {
        36: "Production Deployment Scripts",
        37: "Configuration Management", 
        38: "Production Monitoring",
        39: "Production Logging",
        40: "Backup Systems",
        41: "Security System",
        42: "Auto-scaling",
        43: "Load Testing Framework",
        44: "Stress Testing",
        45: "Performance Benchmarking",
        46: "Database Optimization",
        47: "Caching System",
        48: "Parallel Processing",
        49: "Integration Testing",
        50: "Production Testing"
    }
    
    # Create task status breakdown
    task_breakdown = []
    for task_id in range(36, 51):
        if task_id in all_task_scores:
            score = all_task_scores[task_id]
            status = 'excellent' if score >= 90 else 'good' if score >= 75 else 'needs_improvement' if score >= 50 else 'critical'
            task_breakdown.append({
                'task_id': task_id,
                'task_name': task_names.get(task_id, f"Task {task_id}"),
                'score': score,
                'status': status
            })
        else:
            task_breakdown.append({
                'task_id': task_id,
                'task_name': task_names.get(task_id, f"Task {task_id}"),
                'score': 0,
                'status': 'not_audited'
            })
    
    # Generate recommendations
    recommendations = []
    critical_issues = []
    
    for task_id, score in all_task_scores.items():
        task_name = task_names.get(task_id, f"Task {task_id}")
        if score < 50:
            critical_issues.append(f"CRITICAL: {task_name} (Task {task_id}) - Score: {score:.1f}%")
        elif score < 75:
            recommendations.append(f"IMPROVE: {task_name} (Task {task_id}) - Score: {score:.1f}%")
    
    # Determine production readiness
    if overall_score >= 85 and tasks_critical == 0:
        production_status = "PRODUCTION_READY"
        production_recommendation = "‚úÖ System is ready for production deployment"
    elif overall_score >= 70 and tasks_critical == 0:
        production_status = "PRODUCTION_READY_WITH_MONITORING"
        production_recommendation = "‚ö†Ô∏è System can be deployed but requires close monitoring"
    elif overall_score >= 50:
        production_status = "NEEDS_IMPROVEMENT"
        production_recommendation = "‚ùå System needs improvement before production deployment"
    else:
        production_status = "NOT_PRODUCTION_READY"
        production_recommendation = "üö® System has critical issues - DO NOT DEPLOY"
    
    # Create final report
    final_report = {
        'report_metadata': {
            'generated_at': datetime.now().isoformat(),
            'report_type': 'GROUP_F_FINAL_COMPREHENSIVE_AUDIT',
            'audit_scope': 'All 15 Group F Production Infrastructure Tasks (36-50)',
            'auditor': 'Amazon Q Production Infrastructure Auditor'
        },
        'executive_summary': {
            'overall_score': overall_score,
            'production_status': production_status,
            'production_recommendation': production_recommendation,
            'total_tasks': total_tasks,
            'tasks_audited': len([t for t in task_breakdown if t['status'] != 'not_audited']),
            'completion_percentage': (len([t for t in task_breakdown if t['status'] != 'not_audited']) / 15) * 100
        },
        'score_distribution': {
            'excellent_tasks': tasks_excellent,
            'good_tasks': tasks_good, 
            'needs_improvement_tasks': tasks_needs_improvement,
            'critical_tasks': tasks_critical,
            'not_audited_tasks': 15 - total_tasks
        },
        'task_breakdown': task_breakdown,
        'detailed_task_results': all_detailed_results,
        'critical_issues': critical_issues,
        'recommendations': recommendations,
        'next_steps': generate_next_steps(production_status, critical_issues, recommendations),
        'audit_methodology': {
            'approach': 'Comprehensive functional testing and code analysis',
            'components_tested': [
                'File existence and structure',
                'Class loading and instantiation', 
                'Method availability and functionality',
                'Configuration file validation',
                'Dependency availability',
                'Report generation and storage',
                'Integration capabilities'
            ],
            'scoring_criteria': {
                'excellent': '90-100% - All components functional',
                'good': '75-89% - Most components functional',
                'needs_improvement': '50-74% - Some components functional',
                'critical': '0-49% - Major functionality missing'
            }
        }
    }
    
    # Write final report
    final_report_path = Path('/home/vivi/pixelated/ai/GROUP_F_FINAL_COMPREHENSIVE_AUDIT_REPORT.json')
    with open(final_report_path, 'w') as f:
        json.dump(final_report, f, indent=2)
    
    # Create summary report
    create_summary_report(final_report)
    
    # Log summary
    logger.critical("üö® FINAL GROUP F AUDIT RESULTS:")
    logger.critical(f"üìä Overall Score: {overall_score:.1f}%")
    logger.critical(f"üéØ Production Status: {production_status}")
    logger.critical(f"üìà Tasks Audited: {len([t for t in task_breakdown if t['status'] != 'not_audited'])}/15")
    logger.critical(f"‚úÖ Excellent: {tasks_excellent} | üëç Good: {tasks_good} | ‚ö†Ô∏è Needs Work: {tasks_needs_improvement} | ‚ùå Critical: {tasks_critical}")
    
    return final_report

def generate_next_steps(production_status, critical_issues, recommendations):
    """Generate next steps based on audit results."""
    if production_status == "PRODUCTION_READY":
        return [
            "‚úÖ Deploy to production environment",
            "‚úÖ Enable comprehensive monitoring and alerting", 
            "‚úÖ Run post-deployment validation tests",
            "‚úÖ Monitor system performance for 24-48 hours",
            "‚úÖ Document deployment procedures and lessons learned"
        ]
    elif production_status == "PRODUCTION_READY_WITH_MONITORING":
        return [
            "‚ö†Ô∏è Deploy with enhanced monitoring enabled",
            "‚ö†Ô∏è Address improvement recommendations within 30 days",
            "‚ö†Ô∏è Implement additional health checks and alerts",
            "‚ö†Ô∏è Schedule regular system health reviews",
            "‚ö†Ô∏è Prepare rollback procedures"
        ]
    elif production_status == "NEEDS_IMPROVEMENT":
        return [
            "üîß Address all improvement recommendations",
            "üîß Re-run comprehensive audit after fixes",
            "üîß Implement missing functionality",
            "üîß Enhance system monitoring and logging",
            "üîß Schedule follow-up audit in 2 weeks"
        ]
    else:
        return [
            "üö® DO NOT DEPLOY - Fix critical issues immediately",
            "üö® Address all critical issues before proceeding",
            "üö® Implement comprehensive testing",
            "üö® Re-audit entire system after fixes",
            "üö® Consider system redesign if issues persist"
        ]

def create_summary_report(final_report):
    """Create human-readable summary report."""
    summary_content = f"""
# GROUP F PRODUCTION INFRASTRUCTURE - FINAL AUDIT REPORT

**Generated:** {final_report['report_metadata']['generated_at']}  
**Overall Score:** {final_report['executive_summary']['overall_score']:.1f}%  
**Production Status:** {final_report['executive_summary']['production_status']}  

## EXECUTIVE SUMMARY

{final_report['executive_summary']['production_recommendation']}

**Tasks Audited:** {final_report['executive_summary']['tasks_audited']}/15 ({final_report['executive_summary']['completion_percentage']:.1f}%)

## SCORE BREAKDOWN

- ‚úÖ **Excellent (90-100%):** {final_report['score_distribution']['excellent_tasks']} tasks
- üëç **Good (75-89%):** {final_report['score_distribution']['good_tasks']} tasks  
- ‚ö†Ô∏è **Needs Improvement (50-74%):** {final_report['score_distribution']['needs_improvement_tasks']} tasks
- ‚ùå **Critical (0-49%):** {final_report['score_distribution']['critical_tasks']} tasks
- ‚ùì **Not Audited:** {final_report['score_distribution']['not_audited_tasks']} tasks

## TASK RESULTS

"""
    
    for task in final_report['task_breakdown']:
        status_emoji = {
            'excellent': '‚úÖ',
            'good': 'üëç', 
            'needs_improvement': '‚ö†Ô∏è',
            'critical': '‚ùå',
            'not_audited': '‚ùì'
        }.get(task['status'], '‚ùì')
        
        summary_content += f"- {status_emoji} **Task {task['task_id']}:** {task['task_name']} - {task['score']:.1f}%\n"
    
    if final_report['critical_issues']:
        summary_content += "\n## CRITICAL ISSUES\n\n"
        for issue in final_report['critical_issues']:
            summary_content += f"- üö® {issue}\n"
    
    if final_report['recommendations']:
        summary_content += "\n## RECOMMENDATIONS\n\n"
        for rec in final_report['recommendations']:
            summary_content += f"- ‚ö†Ô∏è {rec}\n"
    
    summary_content += "\n## NEXT STEPS\n\n"
    for step in final_report['next_steps']:
        summary_content += f"- {step}\n"
    
    # Write summary
    summary_path = Path('/home/vivi/pixelated/ai/GROUP_F_AUDIT_SUMMARY.md')
    with open(summary_path, 'w') as f:
        f.write(summary_content)
    
    logger.info(f"‚úÖ Summary report created: {summary_path}")

if __name__ == "__main__":
    create_final_audit_report()
