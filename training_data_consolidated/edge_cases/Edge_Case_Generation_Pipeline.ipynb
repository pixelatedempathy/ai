{"cells":[{"cell_type":"markdown","metadata":{"id":"BhvDj3L9_4Rn"},"source":["# Pixelated Empathy: Edge Case Generation Pipeline\n","\n","This notebook generates challenging therapy scenarios for training difficult client simulation models.\n","\n","---\n","\n","**Instructions:**\n","1. Run each cell in order.\n","2. Configure your API key and model.\n","3. Generate prompts and conversations.\n","4. Analyze and export results.\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nEE3v5su_4Rq"},"outputs":[],"source":["# Install requirements if needed (uncomment if running in Colab)\n","!pip install openai anthropic pandas numpy tqdm matplotlib seaborn ipywidgets requests"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LUBI4eV_4Rr"},"outputs":[],"source":["import os\n","import sys\n","import json\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","from datetime import datetime\n","import ipywidgets as widgets\n","from IPython.display import display, HTML, clear_output\n","\n","# Import the edge case generator\n","from edge_case_generator import EdgeCaseGenerator\n","\n","plt.style.use('default')\n","sns.set_palette(\"husl\")\n","print(\"‚úÖ Imports successful!\")"]},{"cell_type":"markdown","metadata":{"id":"5HUE_abN_4Rs"},"source":["## üîß Configuration\n","\n","Choose your API provider and configure settings:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsGTKVQa_4Rs"},"outputs":[],"source":["api_provider_widget = widgets.Dropdown(\n","    options=['openai', 'anthropic', 'ollama'],\n","    value='ollama',\n","    description='API Provider:',\n","    style={'description_width': 'initial'}\n",")\n","api_key_widget = widgets.Password(\n","    placeholder='Enter your API key (not needed for Ollama)',\n","    description='API Key:',\n","    style={'description_width': 'initial'}\n",")\n","model_widget = widgets.Dropdown(\n","    options={\n","        'GPT-3.5 Turbo': 'gpt-3.5-turbo',\n","        'GPT-4': 'gpt-4',\n","        'GPT-4 Turbo': 'gpt-4-turbo-preview',\n","        'Claude 3 Haiku': 'claude-3-haiku-20240307',\n","        'Claude 3 Sonnet': 'claude-3-sonnet-20240229',\n","        'Claude 3 Opus': 'claude-3-opus-20240229',\n","        'Custom Ollama Model': 'artifish/llama3.2-uncensored'\n","    },\n","    value='gpt-3.5-turbo',\n","    description='Model:',\n","    style={'description_width': 'initial'}\n",")\n","custom_model_widget = widgets.Text(\n","    placeholder='Enter custom model name (for Ollama)',\n","    description='Custom Model:',\n","    style={'description_width': 'initial'}\n",")\n","scenarios_per_category_widget = widgets.IntSlider(\n","    value=20,\n","    min=1,\n","    max=50,\n","    step=1,\n","    description='Scenarios per category:',\n","    style={'description_width': 'initial'}\n",")\n","max_conversations_widget = widgets.IntSlider(\n","    value=100,\n","    min=10,\n","    max=500,\n","    step=10,\n","    description='Max conversations to generate:',\n","    style={'description_width': 'initial'}\n",")\n","output_dir_widget = widgets.Text(\n","    value='edge_case_output',\n","    description='Output directory:',\n","    style={'description_width': 'initial'}\n",")\n","display(widgets.VBox([\n","    widgets.HTML(\"<h3>üîß Configuration</h3>\"),\n","    api_provider_widget,\n","    api_key_widget,\n","    model_widget,\n","    custom_model_widget,\n","    scenarios_per_category_widget,\n","    max_conversations_widget,\n","    output_dir_widget\n","]))"]},{"cell_type":"markdown","metadata":{"id":"6X98pj-Z_4Rt"},"source":["## üìä Preview Edge Case Categories\n","\n","Let's see what categories we'll be generating:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3_7-4fk_4Rt"},"outputs":[],"source":["temp_generator = EdgeCaseGenerator()\n","categories = temp_generator.edge_case_categories\n","category_df = pd.DataFrame([\n","    {\n","        'Category': cat,\n","        'Description': details['description'],\n","        'Difficulty': details['difficulty'],\n","        'Challenges': ', '.join(details['challenges'][:2]) + '...' if len(details['challenges']) > 2 else ', '.join(details['challenges'])\n","    }\n","    for cat, details in categories.items()\n","])\n","print(f\"üìã Total Categories: {len(category_df)}\")\n","print(f\"üéØ Total Scenarios to Generate: {len(category_df) * scenarios_per_category_widget.value}\")\n","display(HTML(category_df.to_html(index=False, escape=False)))\n","fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n","difficulty_counts = category_df['Difficulty'].value_counts()\n","ax[0].pie(difficulty_counts.values, labels=difficulty_counts.index, autopct='%1.1f%%')\n","ax[0].set_title('Difficulty Level Distribution')\n","sns.countplot(data=category_df, x='Difficulty', ax=ax[1])\n","ax[1].set_title('Categories by Difficulty Level')\n","ax[1].set_xlabel('Difficulty Level')\n","ax[1].set_ylabel('Number of Categories')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jR4V7AJe_4Rt"},"source":["## üöÄ Initialize Generator\n","\n","Set up the generator with your chosen configuration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7q2xZfF_4Ru"},"outputs":[],"source":["api_provider = api_provider_widget.value\n","api_key = api_key_widget.value if api_key_widget.value else None\n","model_name = custom_model_widget.value if model_widget.value == 'custom' else model_widget.value\n","scenarios_per_category = scenarios_per_category_widget.value\n","max_conversations = max_conversations_widget.value\n","output_dir = output_dir_widget.value\n","print(f\"üîß Configuration:\")\n","print(f\"   API Provider: {api_provider}\")\n","print(f\"   Model: {model_name}\")\n","print(f\"   Scenarios per category: {scenarios_per_category}\")\n","print(f\"   Max conversations: {max_conversations}\")\n","print(f\"   Output directory: {output_dir}\")\n","try:\n","    generator = EdgeCaseGenerator(\n","        api_provider=api_provider,\n","        api_key=api_key,\n","        model_name=model_name,\n","        output_dir=output_dir\n","    )\n","    print(\"‚úÖ Generator initialized successfully!\")\n","except Exception as e:\n","    print(f\"‚ùå Error initializing generator: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"dhJgLphj_4Ru"},"source":["## üìù Step 1: Generate Prompts\n","\n","First, we'll generate all the prompts for our edge cases:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhCx5b7S_4Ru"},"outputs":[],"source":["print(\"üìù Generating prompts...\")\n","prompts = generator.generate_prompts(scenarios_per_category=scenarios_per_category)\n","print(f\"‚úÖ Generated {len(prompts)} prompts\")\n","print(f\"üìÅ Saved to: {generator.output_dir}/edge_case_prompts.jsonl\")\n","print(\"\n","üìã Sample prompts:\")\n","for i, prompt in enumerate(prompts[:3]):\n","    print(f\"\n","{i+1}. {prompt['scenario_id']} ({prompt['category']})\")\n","    print(f\"   Difficulty: {prompt['difficulty_level']}\")\n","    print(f\"   Instructions: {prompt['instructions'][:100]}...\")"]},{"cell_type":"markdown","metadata":{"id":"f-sJG4Nd_4Ru"},"source":["## ü§ñ Step 2: Generate Conversations\n","\n","Now we'll use the API to generate realistic therapy conversations:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMmQdJH9_4Ru"},"outputs":[],"source":["from tqdm.notebook import tqdm as tqdm_nb\n","progress_widget = widgets.IntProgress(\n","    value=0,\n","    min=0,\n","    max=min(len(prompts), max_conversations),\n","    description='Progress:',\n","    bar_style='info',\n","    style={'bar_width': '50px'},\n","    orientation='horizontal'\n",")\n","status_widget = widgets.HTML(value=\"Starting generation...\")\n","display(widgets.VBox([progress_widget, status_widget]))\n","print(f\"ü§ñ Generating conversations using {api_provider} ({model_name})...\")\n","print(f\"üìä Target: {min(len(prompts), max_conversations)} conversations\")\n","print(\"\n","‚è≥ This may take a while depending on your API provider and limits...\")\n","conversations = []\n","try:\n","    for i, prompt in enumerate(tqdm_nb(prompts[:max_conversations])):\n","        conv = generator._generate_single_conversation(prompt)\n","        if conv:\n","            conversations.append(conv)\n","        progress_widget.value = i + 1\n","        status_widget.value = f'Generated {i+1}/{min(len(prompts), max_conversations)}'\n","    print(f'\n","‚úÖ Generation completed!')\n","    print(f'üìà Success rate: {len(conversations)/min(len(prompts), max_conversations)*100:.1f}%')\n","except Exception as e:\n","    status_widget.value = f'‚ùå Error: {str(e)}'\n","    print(f'\n","‚ùå Error during generation: {e}')"]},{"cell_type":"markdown","metadata":{"id":"IWCK_cAp_4Rv"},"source":["## üìä Step 3: Analyze Results\n","\n","Let's analyze what we generated:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6iXHZmjh_4Rv"},"outputs":[],"source":["if conversations:\n","    analysis_data = []\n","    total_qa_pairs = 0\n","    for conv in conversations:\n","        qa_count = len(conv.get('qa_pairs', []))\n","        total_qa_pairs += qa_count\n","        analysis_data.append({\n","            'Scenario ID': conv['scenario_id'],\n","            'Category': conv['category'],\n","            'Difficulty': conv['difficulty_level'],\n","            'QA Pairs': qa_count,\n","            'Has Content': qa_count > 0\n","        })\n","    analysis_df = pd.DataFrame(analysis_data)\n","    print(f'üìä Analysis Results:')\n","    print(f'   Total Conversations: {len(conversations)}')\n","    print(f'   Total Q&A Pairs: {total_qa_pairs}')\n","    print(f'   Average Q&A per Conversation: {total_qa_pairs/len(conversations):.1f}')\n","    print(f'   Success Rate: {analysis_df[\"Has Content\"].sum()/len(analysis_df)*100:.1f}%')\n","    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","    category_counts = analysis_df['Category'].value_counts()\n","    category_counts.plot(kind='bar', ax=axes[0,0], color='skyblue')\n","    axes[0,0].set_title('Conversations by Category')\n","    axes[0,0].set_xlabel('Category')\n","    axes[0,0].set_ylabel('Count')\n","    axes[0,0].tick_params(axis='x', rotation=45)\n","    difficulty_counts = analysis_df['Difficulty'].value_counts()\n","    axes[0,1].pie(difficulty_counts.values, labels=difficulty_counts.index, autopct='%1.1f%%')\n","    axes[0,1].set_title('Distribution by Difficulty')\n","    analysis_df['QA Pairs'].hist(bins=20, ax=axes[1,0], color='lightgreen', alpha=0.7)\n","    axes[1,0].set_title('Q&A Pairs per Conversation')\n","    axes[1,0].set_xlabel('Number of Q&A Pairs')\n","    axes[1,0].set_ylabel('Frequency')\n","    success_by_category = analysis_df.groupby('Category')['Has Content'].mean().sort_values(ascending=False)\n","    success_by_category.plot(kind='bar', ax=axes[1,1], color='coral')\n","    axes[1,1].set_title('Success Rate by Category')\n","    axes[1,1].set_xlabel('Category')\n","    axes[1,1].set_ylabel('Success Rate')\n","    axes[1,1].tick_params(axis='x', rotation=45)\n","    plt.tight_layout()\n","    plt.show()\n","    if conversations and conversations[0].get('qa_pairs'):\n","        print('\n","üí¨ Sample Generated Conversation:')\n","        sample_conv = conversations[0]\n","        print(f'Scenario: {sample_conv['scenario_id']} ({sample_conv['category']})')\n","        print(f'Difficulty: {sample_conv['difficulty_level']}')\n","        print('Dialogue:')\n","        for i, qa in enumerate(sample_conv['qa_pairs'][:2]):\n","            print(f'\n","Therapist: {qa['prompt']}')\n","            print(f'Client: {qa['response']}')\n","        if len(sample_conv['qa_pairs']) > 2:\n","            print(f'\n","... ({len(sample_conv['qa_pairs']) - 2} more exchanges)')\n","else:\n","    print('‚ùå No conversations generated. Please check your configuration and try again.')"]},{"cell_type":"markdown","metadata":{"id":"SjP4hjyN_4Rv"},"source":["## üîÑ Step 4: Create Training Format\n","\n","Convert to the format needed for training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWX588NQ_4Rv"},"outputs":[],"source":["if conversations:\n","    print('üîÑ Converting to training format...')\n","    training_data = generator.create_training_format(conversations)\n","    print(f'\n","‚úÖ Created {len(training_data)} training examples')\n","    print(f'üìÅ Saved to: {generator.output_dir}/edge_cases_training_format.jsonl')\n","    if training_data:\n","        print('\n","üìã Sample Training Data:')\n","        sample = training_data[0]\n","        print(f'Category: {sample['category']}')\n","        print(f'Difficulty: {sample['difficulty_level']}')\n","        print(f'Purpose: {sample['purpose']}')\n","        print(f'Prompt: {sample['prompt'][:100]}...')\n","        print(f'Response: {sample['response'][:100]}...')\n","else:\n","    print('‚ùå No conversations available for training format conversion.')"]},{"cell_type":"markdown","metadata":{"id":"OCMHKA2x_4Rv"},"source":["## üìÑ Step 5: Generate Summary Report\n","\n","Create a comprehensive report of the generation process:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xI83wpv1_4Rv"},"outputs":[],"source":["if conversations:\n","    print('üìÑ Generating summary report...')\n","    report = generator.generate_summary_report(conversations)\n","    print(f'\n","‚úÖ Report generated and saved to: {generator.output_dir}/summary_report.md')\n","    print('\n","' + '='*60)\n","    print(report)\n","    print('='*60)\n","else:\n","    print('‚ùå No conversations available for report generation.')"]},{"cell_type":"markdown","metadata":{"id":"Bg5thlLf_4Rv"},"source":["## üìÅ File Management\n","\n","View and manage your generated files:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4WeZVBs_4Rv"},"outputs":[],"source":["output_path = Path(output_dir)\n","if output_path.exists():\n","    files = list(output_path.glob('*'))\n","    print(f'üìÅ Files in {output_path}:')\n","    print('-' * 50)\n","    total_size = 0\n","    for file in sorted(files):\n","        if file.is_file():\n","            size = file.stat().st_size\n","            total_size += size\n","            print(f'üìÑ {file.name:<30} {size:>10,} bytes')\n","    print('-' * 50)\n","    print(f'üìä Total size: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)')\n","    import zipfile\n","    zip_path = Path(f'{output_dir}_complete.zip')\n","    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for file in files:\n","            if file.is_file():\n","                zipf.write(file, file.name)\n","    print(f'\n","üì¶ Created zip file: {zip_path}')\n","    print(f'üíæ Zip size: {zip_path.stat().st_size:,} bytes')\n","else:\n","    print(f'‚ùå Output directory {output_path} not found')"]},{"cell_type":"markdown","metadata":{"id":"TgSi5RU__4Rv"},"source":["## üéØ Next Steps\n","\n","- Review sample conversations for realism and appropriateness\n","- Integrate with your main training pipeline\n","- Use in evaluation framework\n","- Download results before your Colab session ends!\n","\n","---\n","\n","**Happy generating! ü§ñ‚ú®**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}