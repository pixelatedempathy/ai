#!/usr/bin/env python3
"""
Quality Assurance Workflows - Task 5.7.2.2
Builds comprehensive quality assurance workflows and processes for conversation validation.
"""

import json
import logging
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
import threading
import queue
import time

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WorkflowStatus(Enum):
    """Workflow execution status"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class QualityLevel(Enum):
    """Quality assessment levels"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    NEEDS_IMPROVEMENT = "needs_improvement"
    UNACCEPTABLE = "unacceptable"

@dataclass
class WorkflowStep:
    """Individual workflow step definition"""
    step_id: str
    name: str
    description: str
    function: Callable
    required: bool = True
    timeout_seconds: int = 300
    retry_count: int = 3
    dependencies: List[str] = None

@dataclass
class WorkflowResult:
    """Result from workflow execution"""
    workflow_id: str
    conversation_id: str
    status: WorkflowStatus
    steps_completed: List[str]
    steps_failed: List[str]
    quality_level: QualityLevel
    quality_score: float
    issues_found: List[str]
    recommendations: List[str]
    execution_time: float
    start_time: str
    end_time: str
    metadata: Dict[str, Any]

class QualityAssuranceWorkflow:
    """Enterprise-grade quality assurance workflow system"""
    
    def __init__(self):
        """Initialize QA workflow system"""
        self.workflows = {}
        self.workflow_history = []
        self.active_workflows = {}
        self.workflow_stats = {
            'total_executed': 0,
            'successful_executions': 0,
            'failed_executions': 0,
            'average_execution_time': 0.0,
            'quality_distribution': {level.value: 0 for level in QualityLevel}
        }
        self._setup_default_workflows()
        
    def _setup_default_workflows(self):
        """Setup default quality assurance workflows"""
        
        # Standard Quality Assurance Workflow
        standard_qa_steps = [
            WorkflowStep(
                step_id="content_validation",
                name="Content Validation",
                description="Validate conversation content and structure",
                function=self._validate_content
            ),
            WorkflowStep(
                step_id="clinical_standards",
                name="Clinical Standards Check",
                description="Validate against clinical standards and guidelines",
                function=self._check_clinical_standards
            ),
            WorkflowStep(
                step_id="safety_assessment",
                name="Safety Assessment",
                description="Assess conversation safety and risk factors",
                function=self._assess_safety
            ),
            WorkflowStep(
                step_id="quality_metrics",
                name="Quality Metrics Calculation",
                description="Calculate comprehensive quality metrics",
                function=self._calculate_quality_metrics
            ),
            WorkflowStep(
                step_id="recommendation_generation",
                name="Recommendation Generation",
                description="Generate improvement recommendations",
                function=self._generate_recommendations
            )
        ]
        
        self.register_workflow("standard_qa", "Standard Quality Assurance", standard_qa_steps)
        
        # Comprehensive Quality Assurance Workflow
        comprehensive_qa_steps = standard_qa_steps + [
            WorkflowStep(
                step_id="linguistic_analysis",
                name="Linguistic Analysis",
                description="Perform detailed linguistic and semantic analysis",
                function=self._perform_linguistic_analysis
            ),
            WorkflowStep(
                step_id="therapeutic_effectiveness",
                name="Therapeutic Effectiveness",
                description="Assess therapeutic effectiveness and outcomes",
                function=self._assess_therapeutic_effectiveness
            ),
            WorkflowStep(
                step_id="cultural_sensitivity",
                name="Cultural Sensitivity Check",
                description="Validate cultural sensitivity and inclusivity",
                function=self._check_cultural_sensitivity
            ),
            WorkflowStep(
                step_id="ethical_compliance",
                name="Ethical Compliance Review",
                description="Review ethical compliance and professional standards",
                function=self._review_ethical_compliance
            )
        ]
        
        self.register_workflow("comprehensive_qa", "Comprehensive Quality Assurance", comprehensive_qa_steps)
        
        # Rapid Quality Check Workflow
        rapid_qa_steps = [
            WorkflowStep(
                step_id="basic_validation",
                name="Basic Validation",
                description="Basic conversation validation and structure check",
                function=self._basic_validation,
                timeout_seconds=60
            ),
            WorkflowStep(
                step_id="safety_screening",
                name="Safety Screening",
                description="Rapid safety and risk screening",
                function=self._safety_screening,
                timeout_seconds=60
            ),
            WorkflowStep(
                step_id="quality_scoring",
                name="Quality Scoring",
                description="Basic quality score calculation",
                function=self._basic_quality_scoring,
                timeout_seconds=60
            )
        ]
        
        self.register_workflow("rapid_qa", "Rapid Quality Check", rapid_qa_steps)
        
        logger.info("‚úÖ Setup default QA workflows: standard_qa, comprehensive_qa, rapid_qa")
    
    def register_workflow(self, workflow_id: str, name: str, steps: List[WorkflowStep]):
        """Register a new quality assurance workflow"""
        self.workflows[workflow_id] = {
            'id': workflow_id,
            'name': name,
            'steps': steps,
            'created_at': datetime.now().isoformat(),
            'execution_count': 0
        }
        logger.info(f"‚úÖ Registered workflow: {workflow_id} with {len(steps)} steps")
    
    def execute_workflow(self, workflow_id: str, conversation: Dict[str, Any], 
                        options: Dict[str, Any] = None) -> WorkflowResult:
        """Execute a quality assurance workflow"""
        
        if workflow_id not in self.workflows:
            raise ValueError(f"Workflow {workflow_id} not found")
        
        workflow = self.workflows[workflow_id]
        conversation_id = conversation.get('id', 'unknown')
        execution_id = f"{workflow_id}_{conversation_id}_{int(time.time())}"
        
        start_time = datetime.now()
        
        logger.info(f"üîÑ Starting workflow {workflow_id} for conversation {conversation_id}")
        
        # Initialize workflow result
        result = WorkflowResult(
            workflow_id=workflow_id,
            conversation_id=conversation_id,
            status=WorkflowStatus.IN_PROGRESS,
            steps_completed=[],
            steps_failed=[],
            quality_level=QualityLevel.ACCEPTABLE,
            quality_score=0.0,
            issues_found=[],
            recommendations=[],
            execution_time=0.0,
            start_time=start_time.isoformat(),
            end_time="",
            metadata=options or {}
        )
        
        self.active_workflows[execution_id] = result
        
        try:
            # Execute workflow steps
            workflow_context = {
                'conversation': conversation,
                'options': options or {},
                'results': {},
                'issues': [],
                'recommendations': []
            }
            
            for step in workflow['steps']:
                step_start = time.time()
                
                try:
                    logger.info(f"  üîÑ Executing step: {step.name}")
                    
                    # Check dependencies
                    if step.dependencies:
                        missing_deps = [dep for dep in step.dependencies 
                                      if dep not in result.steps_completed]
                        if missing_deps:
                            raise Exception(f"Missing dependencies: {missing_deps}")
                    
                    # Execute step with timeout
                    step_result = self._execute_step_with_timeout(
                        step, workflow_context, step.timeout_seconds
                    )
                    
                    # Update context with step results
                    workflow_context['results'][step.step_id] = step_result
                    result.steps_completed.append(step.step_id)
                    
                    step_time = time.time() - step_start
                    logger.info(f"  ‚úÖ Completed step: {step.name} ({step_time:.2f}s)")
                    
                except Exception as e:
                    logger.error(f"  ‚ùå Step failed: {step.name} - {e}")
                    result.steps_failed.append(step.step_id)
                    
                    if step.required:
                        raise Exception(f"Required step {step.name} failed: {e}")
                    else:
                        logger.warning(f"  ‚ö†Ô∏è Optional step {step.name} failed, continuing...")
            
            # Calculate final results
            result.quality_score = self._calculate_final_quality_score(workflow_context)
            result.quality_level = self._determine_quality_level(result.quality_score)
            result.issues_found = workflow_context['issues']
            result.recommendations = workflow_context['recommendations']
            result.status = WorkflowStatus.COMPLETED
            
            logger.info(f"‚úÖ Workflow {workflow_id} completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Workflow {workflow_id} failed: {e}")
            result.status = WorkflowStatus.FAILED
            result.metadata['error'] = str(e)
        
        finally:
            end_time = datetime.now()
            result.end_time = end_time.isoformat()
            result.execution_time = (end_time - start_time).total_seconds()
            
            # Update statistics
            self._update_workflow_stats(result)
            
            # Remove from active workflows
            if execution_id in self.active_workflows:
                del self.active_workflows[execution_id]
            
            # Add to history
            self.workflow_history.append(result)
            workflow['execution_count'] += 1
        
        return result
    
    def _execute_step_with_timeout(self, step: WorkflowStep, context: Dict[str, Any], 
                                 timeout: int) -> Any:
        """Execute a workflow step with timeout"""
        result_queue = queue.Queue()
        exception_queue = queue.Queue()
        
        def step_executor():
            try:
                result = step.function(context)
                result_queue.put(result)
            except Exception as e:
                exception_queue.put(e)
        
        thread = threading.Thread(target=step_executor)
        thread.daemon = True
        thread.start()
        thread.join(timeout)
        
        if thread.is_alive():
            raise TimeoutError(f"Step {step.name} timed out after {timeout} seconds")
        
        if not exception_queue.empty():
            raise exception_queue.get()
        
        if not result_queue.empty():
            return result_queue.get()
        
        raise Exception(f"Step {step.name} completed without result")
    
    def _validate_content(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Validate conversation content and structure"""
        conversation = context['conversation']
        issues = []
        
        # Check basic structure
        if 'messages' not in conversation and 'conversation' not in conversation:
            issues.append("Missing conversation content")
        
        # Check message structure
        if 'messages' in conversation:
            messages = conversation['messages']
            if not isinstance(messages, list) or len(messages) == 0:
                issues.append("Invalid or empty messages structure")
            
            for i, msg in enumerate(messages):
                if not isinstance(msg, dict):
                    issues.append(f"Message {i} is not a dictionary")
                elif 'content' not in msg:
                    issues.append(f"Message {i} missing content")
        
        # Check conversation length
        text_length = len(str(conversation.get('conversation', '')))
        if text_length < 10:
            issues.append("Conversation too short")
        elif text_length > 50000:
            issues.append("Conversation unusually long")
        
        context['issues'].extend(issues)
        
        return {
            'content_valid': len(issues) == 0,
            'issues_found': len(issues),
            'text_length': text_length
        }
    
    def _check_clinical_standards(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Check against clinical standards"""
        # This would integrate with the clinical standards validator
        # For now, implementing basic checks
        
        conversation = context['conversation']
        text = str(conversation.get('conversation', ''))
        issues = []
        
        # Basic clinical checks
        if 'diagnose' in text.lower() and 'professional' not in text.lower():
            issues.append("Potential inappropriate diagnosis without professional disclaimer")
        
        if any(word in text.lower() for word in ['suicide', 'kill myself', 'end it all']):
            if not any(word in text.lower() for word in ['crisis', 'hotline', 'emergency', '988']):
                issues.append("Crisis content without appropriate resources")
        
        context['issues'].extend(issues)
        
        return {
            'clinical_compliant': len(issues) == 0,
            'issues_found': len(issues)
        }
    
    def _assess_safety(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Assess conversation safety"""
        conversation = context['conversation']
        text = str(conversation.get('conversation', '')).lower()
        safety_score = 1.0
        issues = []
        
        # Check for harmful content
        harmful_patterns = [
            'harm yourself', 'hurt yourself', 'end your life',
            'kill yourself', 'not worth living', 'better off dead'
        ]
        
        for pattern in harmful_patterns:
            if pattern in text:
                safety_score -= 0.2
                issues.append(f"Potentially harmful content: {pattern}")
        
        # Check for inappropriate advice
        inappropriate_patterns = [
            'just get over it', 'think positive', 'others have it worse',
            'stop being dramatic', 'it\'s all in your head'
        ]
        
        for pattern in inappropriate_patterns:
            if pattern in text:
                safety_score -= 0.1
                issues.append(f"Inappropriate therapeutic advice: {pattern}")
        
        context['issues'].extend(issues)
        
        return {
            'safety_score': max(0.0, safety_score),
            'is_safe': safety_score >= 0.7,
            'issues_found': len(issues)
        }
    
    def _calculate_quality_metrics(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate comprehensive quality metrics"""
        results = context['results']
        
        # Aggregate scores from previous steps
        content_score = 1.0 if results.get('content_validation', {}).get('content_valid', False) else 0.5
        clinical_score = 1.0 if results.get('clinical_standards', {}).get('clinical_compliant', False) else 0.6
        safety_score = results.get('safety_assessment', {}).get('safety_score', 0.7)
        
        # Calculate weighted average
        weights = {'content': 0.2, 'clinical': 0.4, 'safety': 0.4}
        quality_score = (
            content_score * weights['content'] +
            clinical_score * weights['clinical'] +
            safety_score * weights['safety']
        )
        
        return {
            'quality_score': quality_score,
            'component_scores': {
                'content': content_score,
                'clinical': clinical_score,
                'safety': safety_score
            }
        }
    
    def _generate_recommendations(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate improvement recommendations"""
        results = context['results']
        recommendations = []
        
        # Content recommendations
        if not results.get('content_validation', {}).get('content_valid', True):
            recommendations.append("Improve conversation structure and content quality")
        
        # Clinical recommendations
        if not results.get('clinical_standards', {}).get('clinical_compliant', True):
            recommendations.append("Review clinical standards and professional guidelines")
        
        # Safety recommendations
        safety_score = results.get('safety_assessment', {}).get('safety_score', 1.0)
        if safety_score < 0.8:
            recommendations.append("Address safety concerns and implement crisis protocols")
        
        # Quality recommendations
        quality_score = results.get('quality_metrics', {}).get('quality_score', 0.0)
        if quality_score < 0.7:
            recommendations.append("Overall quality improvement needed")
        
        context['recommendations'].extend(recommendations)
        
        return {
            'recommendations_generated': len(recommendations),
            'recommendations': recommendations
        }
    
    def _perform_linguistic_analysis(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Perform detailed linguistic analysis"""
        # Placeholder for comprehensive linguistic analysis
        return {'linguistic_score': 0.8, 'complexity_score': 0.7}
    
    def _assess_therapeutic_effectiveness(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Assess therapeutic effectiveness"""
        # Placeholder for therapeutic effectiveness assessment
        return {'effectiveness_score': 0.75, 'therapeutic_techniques': 3}
    
    def _check_cultural_sensitivity(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Check cultural sensitivity"""
        # Placeholder for cultural sensitivity check
        return {'cultural_sensitivity_score': 0.85, 'inclusive_language': True}
    
    def _review_ethical_compliance(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Review ethical compliance"""
        # Placeholder for ethical compliance review
        return {'ethical_score': 0.9, 'compliance_issues': 0}
    
    def _basic_validation(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Basic conversation validation"""
        return self._validate_content(context)
    
    def _safety_screening(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Rapid safety screening"""
        return self._assess_safety(context)
    
    def _basic_quality_scoring(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Basic quality scoring"""
        return self._calculate_quality_metrics(context)
    
    def _calculate_final_quality_score(self, context: Dict[str, Any]) -> float:
        """Calculate final quality score from all workflow results"""
        results = context['results']
        
        if 'quality_metrics' in results:
            return results['quality_metrics'].get('quality_score', 0.0)
        
        # Fallback calculation
        scores = []
        for step_result in results.values():
            if isinstance(step_result, dict):
                for key, value in step_result.items():
                    if 'score' in key and isinstance(value, (int, float)):
                        scores.append(value)
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _determine_quality_level(self, score: float) -> QualityLevel:
        """Determine quality level from score"""
        if score >= 0.9:
            return QualityLevel.EXCELLENT
        elif score >= 0.8:
            return QualityLevel.GOOD
        elif score >= 0.7:
            return QualityLevel.ACCEPTABLE
        elif score >= 0.5:
            return QualityLevel.NEEDS_IMPROVEMENT
        else:
            return QualityLevel.UNACCEPTABLE
    
    def _update_workflow_stats(self, result: WorkflowResult):
        """Update workflow execution statistics"""
        self.workflow_stats['total_executed'] += 1
        
        if result.status == WorkflowStatus.COMPLETED:
            self.workflow_stats['successful_executions'] += 1
        else:
            self.workflow_stats['failed_executions'] += 1
        
        # Update average execution time
        total = self.workflow_stats['total_executed']
        current_avg = self.workflow_stats['average_execution_time']
        self.workflow_stats['average_execution_time'] = (
            (current_avg * (total - 1) + result.execution_time) / total
        )
        
        # Update quality distribution
        self.workflow_stats['quality_distribution'][result.quality_level.value] += 1
    
    def execute_batch_workflow(self, workflow_id: str, conversations: List[Dict[str, Any]], 
                             options: Dict[str, Any] = None) -> List[WorkflowResult]:
        """Execute workflow on a batch of conversations"""
        results = []
        
        logger.info(f"üîÑ Starting batch workflow {workflow_id} for {len(conversations)} conversations")
        
        for i, conversation in enumerate(conversations):
            try:
                result = self.execute_workflow(workflow_id, conversation, options)
                results.append(result)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"‚úÖ Processed {i + 1}/{len(conversations)} conversations")
                    
            except Exception as e:
                logger.error(f"‚ùå Error processing conversation {i}: {e}")
                continue
        
        logger.info(f"üéØ Batch workflow complete: {len(results)} conversations processed")
        return results
    
    def get_workflow_statistics(self) -> Dict[str, Any]:
        """Get workflow execution statistics"""
        return {
            'workflow_stats': self.workflow_stats,
            'registered_workflows': list(self.workflows.keys()),
            'active_workflows': len(self.active_workflows),
            'workflow_history_count': len(self.workflow_history),
            'statistics_timestamp': datetime.now().isoformat()
        }
    
    def export_workflow_results(self, results: List[WorkflowResult], output_path: str) -> bool:
        """Export workflow results to JSON file"""
        try:
            output_data = {
                'workflow_results': [asdict(result) for result in results],
                'summary_statistics': self.get_workflow_statistics(),
                'export_timestamp': datetime.now().isoformat()
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"‚úÖ Exported workflow results to {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error exporting workflow results: {e}")
            return False

def main():
    """Test quality assurance workflows"""
    qa_system = QualityAssuranceWorkflow()
    
    # Test conversation
    test_conversation = {
        'id': 'test_workflow_001',
        'messages': [
            {'role': 'user', 'content': 'I\'ve been feeling really anxious lately and can\'t sleep.'},
            {'role': 'assistant', 'content': 'I understand you\'re experiencing anxiety and sleep difficulties. These are common concerns that many people face. Let\'s explore some coping strategies and consider whether professional support might be helpful. Can you tell me more about when these feelings started?'}
        ]
    }
    
    # Execute different workflows
    workflows_to_test = ['rapid_qa', 'standard_qa', 'comprehensive_qa']
    
    for workflow_id in workflows_to_test:
        print(f"\nüîÑ Testing {workflow_id} workflow...")
        
        result = qa_system.execute_workflow(workflow_id, test_conversation)
        
        print(f"Status: {result.status.value}")
        print(f"Quality Level: {result.quality_level.value}")
        print(f"Quality Score: {result.quality_score:.3f}")
        print(f"Execution Time: {result.execution_time:.2f}s")
        print(f"Steps Completed: {len(result.steps_completed)}")
        print(f"Issues Found: {len(result.issues_found)}")
        print(f"Recommendations: {len(result.recommendations)}")
    
    # Get statistics
    stats = qa_system.get_workflow_statistics()
    print(f"\nüìä Workflow Statistics:")
    print(f"Total Executed: {stats['workflow_stats']['total_executed']}")
    print(f"Success Rate: {stats['workflow_stats']['successful_executions']}/{stats['workflow_stats']['total_executed']}")
    print(f"Average Execution Time: {stats['workflow_stats']['average_execution_time']:.2f}s")

if __name__ == "__main__":
    main()
