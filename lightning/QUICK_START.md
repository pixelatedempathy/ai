# Quick Start Guide - Therapeutic AI Training

## ðŸš€ One-Command Training

```bash
# Automatic optimization for 12-hour window
python train_optimized.py
```

That's it! The system will:
- âœ… Analyze your dataset
- âœ… Select optimal configuration
- âœ… Ensure completion within 12 hours
- âœ… Train MoE model with LoRA
- âœ… Save checkpoints every 30 minutes

## ðŸ“‹ Prerequisites

```bash
# Install dependencies
pip install -r requirements_moe.txt

# Ensure you have:
# - training_dataset.json (your data)
# - wandb_config.json (logging)
# - safety_config.json (safety rules)
# - training_config.json (base config)
```

## ðŸŽ¯ What You Get

### Model Architecture
- **4 Domain Experts**: Psychology, Mental Health, Bias Detection, General Therapeutic
- **LoRA Fine-tuning**: ~1-2% trainable parameters
- **Extended Context**: 8192 tokens (4x training length)
- **H100 Optimized**: BFloat16, fused optimizer, gradient checkpointing

### Training Features
- **Automatic Optimization**: Fits in 12-hour window
- **Smart Checkpointing**: Every 30 minutes
- **Early Stopping**: 3-epoch patience
- **Real-time Monitoring**: WandB integration
- **Graceful Shutdown**: Saves before time limit

## ðŸ“Š Expected Results

### For 8,000 Samples
- **Training Time**: 4-5 hours
- **Model Size**: ~1.5GB (LoRA adapters)
- **Memory Usage**: 60GB (H100 has 80GB)
- **Target Loss**: < 1.5
- **Perplexity**: < 2.5

### For 16,000 Samples
- **Training Time**: 8-9 hours
- **Model Size**: ~1.5GB
- **Memory Usage**: 60GB
- **Completes**: Within 12-hour window âœ…

## ðŸ”§ Configuration Options

### In training_config.json

```json
{
  "num_train_epochs": 3,
  "optimization_priority": "balanced",
  "max_training_hours": 12.0
}
```

### Priority Options
- **`fast`**: Fastest training, good quality
- **`balanced`**: Best tradeoff (default)
- **`quality`**: Maximum quality, slower
- **`memory_efficient`**: Lowest memory usage

## ðŸ“ˆ Monitoring

### Console Output
```
ðŸ“Š Progress: 45.2% | Loss: 1.234 | Step: 1500
â° Elapsed: 3.5h | Remaining: 8.5h | On track: âœ…
ðŸ’¾ Checkpoint at 3.5 hours
```

### WandB Dashboard
- Training loss and validation accuracy
- Expert usage distribution
- Time progress and estimates
- Model parameters and memory

## ðŸŽ“ Training Profiles

| Profile | Speed | Quality | Memory | Best For |
|---------|-------|---------|--------|----------|
| Fast | âš¡âš¡âš¡ | â­â­ | 75GB | Large datasets |
| Balanced | âš¡âš¡ | â­â­â­ | 60GB | Most cases |
| Quality | âš¡ | â­â­â­â­ | 70GB | Small datasets |
| Memory Efficient | âš¡ | â­â­ | 45GB | Memory limits |

## ðŸ› Troubleshooting

### Out of Memory
```bash
# Use memory-efficient profile
# Edit training_config.json:
{
  "optimization_priority": "memory_efficient"
}
```

### Training Too Slow
```bash
# Use fast profile
{
  "optimization_priority": "fast"
}
```

### Won't Fit in 12 Hours
The optimizer will automatically:
1. Try faster profile
2. Reduce epochs if needed
3. Adjust batch size
4. Warn you if still won't fit

## ðŸ“ Output Files

After training:
```
therapeutic_moe_model/
â”œâ”€â”€ adapter_config.json          # LoRA config
â”œâ”€â”€ adapter_model.bin            # LoRA weights (~1.5GB)
â”œâ”€â”€ moe_layers.pt                # MoE expert weights
â”œâ”€â”€ tokenizer files              # Tokenizer
â””â”€â”€ checkpoints/                 # Training checkpoints
    â”œâ”€â”€ checkpoint-500/
    â”œâ”€â”€ checkpoint-1000/
    â””â”€â”€ checkpoint-1500/
```

## ðŸŽ¯ Next Steps

After training:
1. **Evaluate**: Test on held-out data
2. **Deploy**: Use deployment scripts
3. **Monitor**: Set up production monitoring
4. **Iterate**: Fine-tune based on results

## ðŸ“š More Information

- **Full Guide**: `MOE_TRAINING_GUIDE.md`
- **Optimization**: `TRAINING_OPTIMIZATION_GUIDE.md`
- **Architecture**: `moe_architecture.py`
- **Training Script**: `train_optimized.py`

## âš¡ Advanced Usage

### Manual Optimization
```python
from training_optimizer import optimize_for_dataset

profile, estimate, args = optimize_for_dataset(
    num_samples=8000,
    avg_tokens_per_sample=500,
    num_epochs=3,
    priority='balanced'
)
```

### Custom Configuration
```python
from moe_architecture import MoEConfig

config = MoEConfig(
    num_experts=4,
    lora_r=16,
    lora_alpha=32,
    max_position_embeddings=8192
)
```

## âœ… Checklist

Before training:
- [ ] Dataset prepared (`training_dataset.json`)
- [ ] WandB configured
- [ ] Dependencies installed
- [ ] GPU available (H100 recommended)

During training:
- [ ] Monitor progress in console
- [ ] Check WandB dashboard
- [ ] Verify checkpoints saving

After training:
- [ ] Model saved successfully
- [ ] Evaluate on test data
- [ ] Review training metrics
- [ ] Plan deployment

---

**Ready to train?** Run `python train_optimized.py` and you're good to go! ðŸš€
