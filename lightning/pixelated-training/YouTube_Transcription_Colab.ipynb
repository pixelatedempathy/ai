{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ðŸŽµ YouTube Video Transcription Pipeline\n",
    "## Pixelated Empathy Training Dataset\n",
    "\n",
    "This notebook transcribes YouTube videos and creates beautiful markdown files for the training dataset.\n",
    "\n",
    "**Features:**\n",
    "- High-quality Whisper transcription\n",
    "- Beautiful markdown formatting\n",
    "- Batch processing with progress tracking\n",
    "- Automatic file organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ðŸ”§ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install openai-whisper tqdm\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import whisper\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload"
   },
   "source": [
    "## ðŸ“ Upload Audio Files\n",
    "\n",
    "**Option 1:** Upload a ZIP file containing all MP3s  \n",
    "**Option 2:** Upload individual MP3 files  \n",
    "**Option 3:** Use files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_files"
   },
   "outputs": [],
   "source": [
    "# Create working directories\n",
    "os.makedirs('/content/audio', exist_ok=True)\n",
    "os.makedirs('/content/transcripts', exist_ok=True)\n",
    "os.makedirs('/content/markdown', exist_ok=True)\n",
    "\n",
    "# Option 1: Upload ZIP file (uncomment to use)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# for filename in uploaded.keys():\n",
    "#     if filename.endswith('.zip'):\n",
    "#         with ZipFile(filename, 'r') as zip_ref:\n",
    "#             zip_ref.extractall('/content/audio')\n",
    "#         print(f\"Extracted {filename}\")\n",
    "\n",
    "# Option 2: Upload individual files (uncomment to use)\n",
    "# uploaded = files.upload()\n",
    "# for filename in uploaded.keys():\n",
    "#     if filename.endswith('.mp3'):\n",
    "#         shutil.move(filename, f'/content/audio/{filename}')\n",
    "\n",
    "# Option 3: Copy from Google Drive (modify path as needed)\n",
    "# drive_path = '/content/drive/MyDrive/youtube_audio'  # Change this path\n",
    "# if os.path.exists(drive_path):\n",
    "#     for file in os.listdir(drive_path):\n",
    "#         if file.endswith('.mp3'):\n",
    "#             shutil.copy2(f'{drive_path}/{file}', f'/content/audio/{file}')\n",
    "\n",
    "# List uploaded files\n",
    "audio_files = [f for f in os.listdir('/content/audio') if f.endswith('.mp3')]\n",
    "print(f\"Found {len(audio_files)} MP3 files\")\n",
    "for f in audio_files[:5]:  # Show first 5\n",
    "    print(f\"  - {f}\")\n",
    "if len(audio_files) > 5:\n",
    "    print(f\"  ... and {len(audio_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transcription"
   },
   "source": [
    "## ðŸŽ¤ Transcription Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load Whisper model\n",
    "MODEL_SIZE = \"base\"  # Options: tiny, base, small, medium, large\n",
    "print(f\"Loading Whisper model: {MODEL_SIZE}\")\n",
    "model = whisper.load_model(MODEL_SIZE)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transcription_functions"
   },
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "    \"\"\"Clean filename for safe usage\"\"\"\n",
    "    name = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "    name = re.sub(r'[^\\w\\s\\-_\\.]', '_', name)\n",
    "    return re.sub(r'\\s+', '_', name)[:200]\n",
    "\n",
    "def extract_metadata(filename):\n",
    "    \"\"\"Extract channel and title from filename\"\"\"\n",
    "    name = filename.replace('.mp3', '')\n",
    "    \n",
    "    # Try to parse channel and title\n",
    "    if ' - ' in name:\n",
    "        parts = name.split(' - ', 1)\n",
    "        channel = parts[0].strip()\n",
    "        title = parts[1].strip()\n",
    "    else:\n",
    "        channel = \"Unknown Channel\"\n",
    "        title = name\n",
    "    \n",
    "    return {\n",
    "        'channel': channel,\n",
    "        'title': title,\n",
    "        'filename': filename,\n",
    "        'processed_date': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def create_markdown(transcription, metadata):\n",
    "    \"\"\"Create beautiful markdown transcript\"\"\"\n",
    "    \n",
    "    markdown = f\"\"\"# {metadata['title']}\n",
    "\n",
    "## ðŸ“º Channel Information\n",
    "**Channel:** {metadata['channel']}  \n",
    "**File:** `{metadata['filename']}`  \n",
    "**Processed:** {metadata['processed_date'][:10]}  \n",
    "\n",
    "## ðŸ“Š Transcript Details\n",
    "- **Language:** {transcription['language'].upper()}\n",
    "- **Duration:** {sum(seg.get('end', 0) - seg.get('start', 0) for seg in transcription.get('segments', [])):.1f} seconds\n",
    "- **Word Count:** {len(transcription['text'].split()):,} words\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Full Transcript\n",
    "\n",
    "{transcription['text']}\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Detailed Segments\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add segments\n",
    "    for i, segment in enumerate(transcription.get('segments', []), 1):\n",
    "        start = segment.get('start', 0)\n",
    "        end = segment.get('end', 0)\n",
    "        text = segment.get('text', '').strip()\n",
    "        \n",
    "        start_min, start_sec = divmod(int(start), 60)\n",
    "        end_min, end_sec = divmod(int(end), 60)\n",
    "        \n",
    "        markdown += f\"\"\"### Segment {i}\n",
    "**Time:** {start_min:02d}:{start_sec:02d} - {end_min:02d}:{end_sec:02d}\n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    markdown += f\"\"\"---\n",
    "\n",
    "## ðŸ”§ Technical Information\n",
    "\n",
    "- **Model:** Whisper {MODEL_SIZE}\n",
    "- **Pipeline:** Pixelated Empathy Training Dataset\n",
    "- **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "---\n",
    "\n",
    "*Transcript generated for Pixelated Empathy AI training dataset.*\n",
    "\"\"\"\n",
    "    \n",
    "    return markdown\n",
    "\n",
    "print(\"Functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process_files"
   },
   "outputs": [],
   "source": [
    "# Process all audio files\n",
    "audio_files = [f for f in os.listdir('/content/audio') if f.endswith('.mp3')]\n",
    "results = {'successful': 0, 'failed': 0, 'files': []}\n",
    "\n",
    "print(f\"Processing {len(audio_files)} files...\\n\")\n",
    "\n",
    "for i, filename in enumerate(tqdm(audio_files, desc=\"Transcribing\")):\n",
    "    try:\n",
    "        # Transcribe\n",
    "        audio_path = f'/content/audio/{filename}'\n",
    "        result = model.transcribe(audio_path)\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = extract_metadata(filename)\n",
    "        \n",
    "        # Create clean filename\n",
    "        clean_name = sanitize_filename(filename.replace('.mp3', ''))\n",
    "        \n",
    "        # Save JSON\n",
    "        json_data = {\n",
    "            'metadata': metadata,\n",
    "            'transcription': result,\n",
    "            'processed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        json_path = f'/content/transcripts/{clean_name}.json'\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save Markdown\n",
    "        markdown_content = create_markdown(result, metadata)\n",
    "        markdown_path = f'/content/markdown/{clean_name}.md'\n",
    "        with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_content)\n",
    "        \n",
    "        results['successful'] += 1\n",
    "        results['files'].append({\n",
    "            'filename': filename,\n",
    "            'status': 'success',\n",
    "            'json_path': json_path,\n",
    "            'markdown_path': markdown_path,\n",
    "            'language': result.get('language', 'unknown'),\n",
    "            'duration': sum(seg.get('end', 0) - seg.get('start', 0) for seg in result.get('segments', [])),\n",
    "            'word_count': len(result['text'].split())\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… {filename} - {result.get('language', 'unknown')} - {len(result['text'].split())} words\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['failed'] += 1\n",
    "        results['files'].append({\n",
    "            'filename': filename,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        })\n",
    "        print(f\"âŒ {filename} - Error: {e}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Processing complete!\")\n",
    "print(f\"âœ… Successful: {results['successful']}\")\n",
    "print(f\"âŒ Failed: {results['failed']}\")\n",
    "print(f\"ðŸ“ˆ Success Rate: {results['successful']/(results['successful']+results['failed'])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## ðŸ“¥ Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_summary"
   },
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'processing_summary': {\n",
    "        'total_files': len(audio_files),\n",
    "        'successful': results['successful'],\n",
    "        'failed': results['failed'],\n",
    "        'success_rate': f\"{results['successful']/(results['successful']+results['failed'])*100:.1f}%\",\n",
    "        'processed_date': datetime.now().isoformat(),\n",
    "        'model_used': MODEL_SIZE\n",
    "    },\n",
    "    'file_details': results['files'],\n",
    "    'statistics': {\n",
    "        'total_words': sum(f.get('word_count', 0) for f in results['files'] if f['status'] == 'success'),\n",
    "        'total_duration': sum(f.get('duration', 0) for f in results['files'] if f['status'] == 'success'),\n",
    "        'languages': list(set(f.get('language', 'unknown') for f in results['files'] if f['status'] == 'success'))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open('/content/transcription_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ðŸ“Š Summary Report:\")\n",
    "print(f\"Total Words: {summary['statistics']['total_words']:,}\")\n",
    "print(f\"Total Duration: {summary['statistics']['total_duration']:.1f} seconds\")\n",
    "print(f\"Languages: {', '.join(summary['statistics']['languages'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_zip"
   },
   "outputs": [],
   "source": [
    "# Create ZIP file for download\n",
    "import zipfile\n",
    "\n",
    "zip_filename = f'pixelated_transcripts_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add all markdown files\n",
    "    for file in os.listdir('/content/markdown'):\n",
    "        if file.endswith('.md'):\n",
    "            zipf.write(f'/content/markdown/{file}', f'markdown/{file}')\n",
    "    \n",
    "    # Add all JSON files\n",
    "    for file in os.listdir('/content/transcripts'):\n",
    "        if file.endswith('.json'):\n",
    "            zipf.write(f'/content/transcripts/{file}', f'json/{file}')\n",
    "    \n",
    "    # Add summary\n",
    "    zipf.write('/content/transcription_summary.json', 'transcription_summary.json')\n",
    "\n",
    "print(f\"ðŸ“¦ Created: {zip_filename}\")\n",
    "print(f\"ðŸ“ Contains: {len(os.listdir('/content/markdown'))} markdown files\")\n",
    "print(f\"ðŸ“„ Contains: {len(os.listdir('/content/transcripts'))} JSON files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_files"
   },
   "outputs": [],
   "source": [
    "# Download the ZIP file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading transcription results...\")\n",
    "files.download(zip_filename)\n",
    "\n",
    "# Also save to Google Drive (optional)\n",
    "drive_path = '/content/drive/MyDrive/pixelated_transcripts'\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "shutil.copy2(zip_filename, f'{drive_path}/{zip_filename}')\n",
    "print(f\"Also saved to Google Drive: {drive_path}/{zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preview"
   },
   "source": [
    "## ðŸ‘€ Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_preview"
   },
   "outputs": [],
   "source": [
    "# Show a sample markdown file\n",
    "markdown_files = [f for f in os.listdir('/content/markdown') if f.endswith('.md')]\n",
    "\n",
    "if markdown_files:\n",
    "    sample_file = markdown_files[0]\n",
    "    print(f\"ðŸ“„ Preview of: {sample_file}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(f'/content/markdown/{sample_file}', 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        # Show first 1000 characters\n",
    "        print(content[:1000])\n",
    "        if len(content) > 1000:\n",
    "            print(\"\\n... (truncated)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Full file size: {len(content):,} characters\")\n",
    "else:\n",
    "    print(\"No markdown files found.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
