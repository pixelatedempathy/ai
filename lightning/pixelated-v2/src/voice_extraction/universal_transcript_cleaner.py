"""Universal transcript cleaner for all transcript directories."""

import re
import os
from pathlib import Path
from typing import List, Dict, Tuple
from src.core.logging import get_logger

logger = get_logger("voice_extraction.universal_transcript_cleaner")


class UniversalTranscriptCleaner:
    """Clean and standardize all transcripts in the notes directory."""
    
    def __init__(self):
        # Enhanced cleaning patterns for all transcript types
        self.cleaning_patterns = [
            # Remove timestamps and markers
            (r'\[\d+:\d+:\d+\]', ''),
            (r'\(\d+:\d+:\d+\)', ''),
            (r'<\d+:\d+:\d+>', ''),
            (r'\d+:\d+:\d+', ''),
            
            # Remove speaker labels and interview artifacts
            (r'^Speaker \d+:\s*', ''),
            (r'^Tim:\s*', ''),
            (r'^Tim Fletcher:\s*', ''),
            (r'^Interviewer:\s*', ''),
            (r'^Host:\s*', ''),
            (r'^Guest:\s*', ''),
            (r'^Dr\.\s+\w+:\s*', ''),
            (r'^\w+:\s*', ''),  # Generic speaker labels
            
            # Remove filler words and sounds
            (r'\b(um|uh|ah|er|hmm|mm|mhm)\b', ''),
            (r'\[inaudible\]', ''),
            (r'\[unclear\]', ''),
            (r'\[crosstalk\]', ''),
            (r'\[laughter\]', ''),
            (r'\[applause\]', ''),
            (r'\[music\]', ''),
            (r'\[pause\]', ''),
            (r'\[silence\]', ''),
            
            # Remove transcription artifacts
            (r'\[TRANSCRIPT\]', ''),
            (r'\[END TRANSCRIPT\]', ''),
            (r'\[AUTO-GENERATED\]', ''),
            (r'Transcript by.*', ''),
            (r'Generated by.*', ''),
            
            # Fix common transcription errors
            (r'\bgonna\b', 'going to'),
            (r'\bwanna\b', 'want to'),
            (r'\bgotta\b', 'got to'),
            (r'\bkinda\b', 'kind of'),
            (r'\bsorta\b', 'sort of'),
            (r'\byeah\b', 'yes'),
            (r'\byep\b', 'yes'),
            (r'\bnah\b', 'no'),
            (r'\bnope\b', 'no'),
            
            # Standardize contractions
            (r"won't", "will not"),
            (r"can't", "cannot"),
            (r"n't", " not"),
            (r"'re", " are"),
            (r"'ve", " have"),
            (r"'ll", " will"),
            (r"'d", " would"),
            (r"'m", " am"),
            
            # Fix spacing and punctuation
            (r'\s+', ' '),  # Multiple spaces to single
            (r'\s*([.!?])\s*', r'\1 '),  # Proper punctuation spacing
            (r'([.!?])\s*([a-z])', r'\1 \2'),  # Space after punctuation
            (r'^\s+|\s+$', ''),  # Leading/trailing whitespace
        ]
        
        # Sentence ending patterns
        self.sentence_endings = r'[.!?]'
        
        # Paragraph break indicators (more comprehensive)
        self.paragraph_breaks = [
            'Now,', 'So,', 'And so,', 'But', 'However,', 'Therefore,',
            'The next thing', 'Another thing', 'Let me', 'I want to',
            'First,', 'Second,', 'Third,', 'Finally,', 'In conclusion,',
            'Moving on,', 'Additionally,', 'Furthermore,', 'Moreover,',
            'On the other hand,', 'In contrast,', 'Similarly,', 'Meanwhile,'
        ]
    
    def clean_single_file(self, file_path: Path) -> Tuple[str, Dict]:
        """Clean a single transcript file."""
        logger.info(f"Cleaning: {file_path.name}")
        
        try:
            # Read file with multiple encoding attempts
            content = self._read_file_safely(file_path)
            if not content:
                return "", {}
            
            original_length = len(content)
            original_lines = len(content.split('\n'))
            
            # Apply cleaning patterns
            cleaned_content = content
            for pattern, replacement in self.cleaning_patterns:
                cleaned_content = re.sub(pattern, replacement, cleaned_content, flags=re.IGNORECASE | re.MULTILINE)
            
            # Fix sentence structure
            cleaned_content = self._fix_sentence_structure(cleaned_content)
            
            # Create proper paragraphs
            cleaned_content = self._create_paragraphs(cleaned_content)
            
            # Final cleanup
            cleaned_content = self._final_cleanup(cleaned_content)
            
            # Generate stats
            stats = {
                'original_length': original_length,
                'cleaned_length': len(cleaned_content),
                'original_lines': original_lines,
                'cleaned_lines': len(cleaned_content.split('\n')),
                'reduction_percent': round((1 - len(cleaned_content) / original_length) * 100, 2) if original_length > 0 else 0,
                'word_count': len(cleaned_content.split()),
                'sentence_count': len(re.findall(self.sentence_endings, cleaned_content))
            }
            
            return cleaned_content, stats
            
        except Exception as e:
            logger.error(f"Failed to clean {file_path}: {e}")
            return "", {}
    
    def _read_file_safely(self, file_path: Path) -> str:
        """Read file with multiple encoding attempts."""
        encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252', 'iso-8859-1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
            except Exception as e:
                logger.warning(f"Error reading {file_path} with {encoding}: {e}")
                continue
        
        logger.error(f"Could not read {file_path} with any encoding")
        return ""
    
    def _fix_sentence_structure(self, text: str) -> str:
        """Fix sentence structure and capitalization."""
        # Split into sentences more carefully
        sentences = re.split(r'([.!?]+)', text)
        
        fixed_sentences = []
        for i in range(0, len(sentences), 2):
            if i < len(sentences):
                sentence = sentences[i].strip()
                if sentence:
                    # Capitalize first letter
                    sentence = sentence[0].upper() + sentence[1:] if len(sentence) > 1 else sentence.upper()
                    
                    # Add punctuation if missing
                    if i + 1 < len(sentences):
                        punctuation = sentences[i + 1]
                    else:
                        punctuation = '.' if not sentence.endswith(('.', '!', '?')) else ''
                    
                    fixed_sentences.append(sentence + punctuation)
        
        return ' '.join(fixed_sentences)
    
    def _create_paragraphs(self, text: str) -> str:
        """Create logical paragraph breaks."""
        sentences = re.split(r'([.!?]+\s*)', text)
        
        paragraphs = []
        current_paragraph = []
        
        for i in range(0, len(sentences), 2):
            if i < len(sentences):
                sentence = sentences[i].strip()
                if sentence:
                    # Check if this sentence should start a new paragraph
                    should_break = any(sentence.startswith(indicator) for indicator in self.paragraph_breaks)
                    
                    if should_break and current_paragraph:
                        # End current paragraph
                        paragraphs.append(' '.join(current_paragraph))
                        current_paragraph = []
                    
                    # Add punctuation
                    punctuation = sentences[i + 1] if i + 1 < len(sentences) else ''
                    current_paragraph.append(sentence + punctuation)
        
        # Add final paragraph
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        return '\n\n'.join(paragraphs)
    
    def _final_cleanup(self, text: str) -> str:
        """Final cleanup pass."""
        # Remove empty lines
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # Join with proper spacing
        text = '\n\n'.join(lines)
        
        # Fix common issues
        text = re.sub(r'\s+([.!?])', r'\1', text)  # Remove space before punctuation
        text = re.sub(r'([.!?])\s*([a-z])', r'\1 \2', text)  # Ensure space after punctuation
        text = re.sub(r'\n\n+', '\n\n', text)  # Max 2 newlines
        
        return text.strip()
    
    def process_directory(self, directory_path: Path, output_dir: Path = None) -> Dict:
        """Process all transcript files in a directory."""
        directory_path = Path(directory_path)
        
        if output_dir is None:
            output_dir = directory_path / "cleaned"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(exist_ok=True)
        
        stats_summary = {
            'directory': str(directory_path),
            'total_files': 0,
            'successful_cleanings': 0,
            'failed_cleanings': 0,
            'total_original_length': 0,
            'total_cleaned_length': 0,
            'total_word_count': 0,
            'total_sentence_count': 0,
            'file_stats': {}
        }
        
        # Process all text files
        for file_path in directory_path.glob("*.txt"):
            stats_summary['total_files'] += 1
            
            cleaned_content, file_stats = self.clean_single_file(file_path)
            
            if cleaned_content and file_stats:
                # Save cleaned file
                output_file = output_dir / f"cleaned_{file_path.name}"
                try:
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(cleaned_content)
                    
                    # Update summary stats
                    stats_summary['successful_cleanings'] += 1
                    stats_summary['total_original_length'] += file_stats['original_length']
                    stats_summary['total_cleaned_length'] += file_stats['cleaned_length']
                    stats_summary['total_word_count'] += file_stats['word_count']
                    stats_summary['total_sentence_count'] += file_stats['sentence_count']
                    stats_summary['file_stats'][file_path.name] = file_stats
                    
                    logger.info(f"âœ… {file_path.name}: {file_stats['word_count']} words")
                    
                except Exception as e:
                    logger.error(f"Failed to save {output_file}: {e}")
                    stats_summary['failed_cleanings'] += 1
            else:
                stats_summary['failed_cleanings'] += 1
        
        # Process markdown files too
        for file_path in directory_path.glob("*.md"):
            stats_summary['total_files'] += 1
            
            cleaned_content, file_stats = self.clean_single_file(file_path)
            
            if cleaned_content and file_stats:
                # Save as .txt file
                output_file = output_dir / f"cleaned_{file_path.stem}.txt"
                try:
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(cleaned_content)
                    
                    # Update summary stats
                    stats_summary['successful_cleanings'] += 1
                    stats_summary['total_original_length'] += file_stats['original_length']
                    stats_summary['total_cleaned_length'] += file_stats['cleaned_length']
                    stats_summary['total_word_count'] += file_stats['word_count']
                    stats_summary['total_sentence_count'] += file_stats['sentence_count']
                    stats_summary['file_stats'][file_path.name] = file_stats
                    
                    logger.info(f"âœ… {file_path.name}: {file_stats['word_count']} words")
                    
                except Exception as e:
                    logger.error(f"Failed to save {output_file}: {e}")
                    stats_summary['failed_cleanings'] += 1
            else:
                stats_summary['failed_cleanings'] += 1
        
        # Calculate overall stats
        if stats_summary['total_original_length'] > 0:
            stats_summary['overall_reduction_percent'] = round(
                (1 - stats_summary['total_cleaned_length'] / stats_summary['total_original_length']) * 100, 2
            )
        
        logger.info(f"Directory complete: {stats_summary['successful_cleanings']}/{stats_summary['total_files']} files")
        
        return stats_summary
    
    def process_all_transcripts(self, base_dir: str = "/root/pixelated/.notes/transcripts") -> Dict:
        """Process all transcript directories."""
        base_path = Path(base_dir)
        all_stats = {
            'base_directory': str(base_path),
            'directories_processed': 0,
            'total_files': 0,
            'total_successful': 0,
            'total_failed': 0,
            'total_words': 0,
            'directory_stats': {}
        }
        
        # Process each subdirectory
        for item in base_path.iterdir():
            if item.is_dir():
                logger.info(f"Processing directory: {item.name}")
                
                dir_stats = self.process_directory(item)
                all_stats['directories_processed'] += 1
                all_stats['total_files'] += dir_stats['total_files']
                all_stats['total_successful'] += dir_stats['successful_cleanings']
                all_stats['total_failed'] += dir_stats['failed_cleanings']
                all_stats['total_words'] += dir_stats['total_word_count']
                all_stats['directory_stats'][item.name] = dir_stats
        
        # Also process files in the root directory
        root_stats = self.process_directory(base_path, base_path / "cleaned_root")
        if root_stats['total_files'] > 0:
            all_stats['total_files'] += root_stats['total_files']
            all_stats['total_successful'] += root_stats['successful_cleanings']
            all_stats['total_failed'] += root_stats['failed_cleanings']
            all_stats['total_words'] += root_stats['total_word_count']
            all_stats['directory_stats']['_root'] = root_stats
        
        logger.info(f"ðŸŽ¯ COMPLETE: {all_stats['total_successful']}/{all_stats['total_files']} files, {all_stats['total_words']:,} words")
        
        return all_stats


def clean_all_transcripts():
    """Main function to clean all transcripts."""
    cleaner = UniversalTranscriptCleaner()
    
    # Process all transcript directories
    stats = cleaner.process_all_transcripts()
    
    # Save overall stats
    import json
    output_file = Path("/root/pixelated/.notes/transcripts/universal_cleaning_stats.json")
    with open(output_file, 'w') as f:
        json.dump(stats, f, indent=2)
    
    logger.info(f"Stats saved to: {output_file}")
    
    return stats


if __name__ == "__main__":
    clean_all_transcripts()
